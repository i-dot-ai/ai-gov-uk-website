<!DOCTYPE html><html lang="en" data-astro-cid-sckkx6r4> <head><meta charset="utf-8"><meta content="width=device-width, initial-scale=1.0" name="viewport"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><link rel="preload" href="/fonts/static/Inter-Regular.ttf" as="font" type="font/ttf" crossorigin="anonymous"><link rel="preload" href="/fonts/static/Inter-Medium.ttf" as="font" type="font/ttf" crossorigin="anonymous"><title>
			Security and risks - 
			AI Knowledge Hub
		</title><meta property="og:title" content="Security and risks"><meta property="og:type" content="website"><meta property="og:url" content="https://ai.gov.uk/knowledge-hub/"><meta property="og:description" content="Find tools, prompts and how-to guides for teams adopting AI technologies in the UK government"><meta property="og:site_name" content="AI Knowledge Hub"><meta property="og:image" content="https://ai.gov.uk/img/knowledge-hub-banner.png"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@i_dot_ai"><meta name="twitter:title" content="Security and risks"><meta name="twitter:description" content="Find tools, prompts and how-to guides for teams adopting AI technologies in the UK government"><meta name="twitter:image" content="https://ai.gov.uk/img/knowledge-hub-banner.png"><link href="/img/favicon.ico" rel="icon"><link href="/img/apple-touch-icon.png" rel="apple-touch-icon"><link rel="stylesheet" href="https://rsms.me/inter/inter.css"><script type="module" src="https://cdn.jsdelivr.net/gh/lit/dist@3/core/lit-core.min.js"></script><script type="module" src="/_astro/Layout.astro_astro_type_script_index_0_lang.uKs0OfS_.js"></script><script defer data-domain="ai.gov.uk" src="https://plausible.io/js/script.pageview-props.tagged-events.outbound-links.file-downloads.js"></script><script>(function(){const posthogKey = "phc_HQYS98bU7UxEA581h5DytBmvbncBVR79TGuYZQO5mds";

			!function(t,e){var o,n,p,r;e.__SV||(window.posthog=e,e._i=[],e.init=function(i,s,a){function g(t,e){var o=e.split(".");2==o.length&&(t=t[o[0]],e=o[1]),t[e]=function(){t.push([e].concat(Array.prototype.slice.call(arguments,0)))}}(p=t.createElement("script")).type="text/javascript",p.crossOrigin="anonymous",p.async=!0,p.src=s.api_host.replace(".i.posthog.com","-assets.i.posthog.com")+"/static/array.js",(r=t.getElementsByTagName("script")[0]).parentNode.insertBefore(p,r);var u=e;for(void 0!==a?u=e[a]=[]:a="posthog",u.people=u.people||[],u.toString=function(t){var e="posthog";return"posthog"!==a&&(e+="."+a),t||(e+=" (stub)"),e},u.people.toString=function(){return u.toString(1)+".people (stub)"},o="init Te Ds js Re Os As capture Ke calculateEventProperties zs register register_once register_for_session unregister unregister_for_session Hs getFeatureFlag getFeatureFlagPayload isFeatureEnabled reloadFeatureFlags updateEarlyAccessFeatureEnrollment getEarlyAccessFeatures on onFeatureFlags onSurveysLoaded onSessionId getSurveys getActiveMatchingSurveys renderSurvey displaySurvey canRenderSurvey canRenderSurveyAsync identify setPersonProperties group resetGroups setPersonPropertiesForFlags resetPersonPropertiesForFlags setGroupPropertiesForFlags resetGroupPropertiesForFlags reset get_distinct_id getGroups get_session_id get_session_replay_url alias set_config startSessionRecording stopSessionRecording sessionRecordingStarted captureException loadToolbar get_property getSessionProperty qs Ns createPersonProfile Bs Cs Ws opt_in_capturing opt_out_capturing has_opted_in_capturing has_opted_out_capturing get_explicit_consent_status is_capturing clear_opt_in_out_capturing Ls debug L Us getPageViewId captureTraceFeedback captureTraceMetric".split(" "),n=0;n<o.length;n++)g(u,o[n]);e._i.push([i,s,a])},e.__SV=1)}(document,window.posthog||[]);
			posthog.init(posthogKey, { //pragma: allowlist secret
				cookieless_mode: 'always',	
				api_host: 'https://eu.i.posthog.com',
				defaults: '2025-05-24',
			})
		})();</script><link rel="stylesheet" href="/_astro/about.C3ZOaQH8.css">
<style>.kh-back-to-top[data-astro-cid-qrl4wijt]{display:none}.kh-back-to-top__icon[data-astro-cid-qrl4wijt]{margin-right:10px}@media (min-width: 641px){.kh-back-to-top[data-astro-cid-qrl4wijt]{display:block;flex-shrink:0;padding-top:1.5rem;padding-bottom:.5rem}}
</style></head> <body class="govuk-template__body govuk-frontend-supported" data-astro-cid-sckkx6r4> <a class="govuk-skip-link" data-module="govuk-skip-link" href="#main" data-astro-cid-sckkx6r4>Skip to main content</a> <header> <div class="govuk-header govuk-header--full-width-border" data-module="govuk-header"> <div class="govuk-header__container govuk-width-container"> <div class="govuk-header__logo"> <a href="/" class="govuk-header__link govuk-header__link--homepage"> <span style="font-weight: bold;">AI.GOV.UK</span> <span class="govuk-header__product-name">Knowledge Hub</span> </a> </div> <div class="govuk-header__content"> <nav aria-label="Menu" class="govuk-header__navigation"> <button id="header-toggle-button" type="button" class="govuk-header__menu-button govuk-js-header-toggle" aria-controls="navigation" aria-expanded="false">Menu</button> <ul id="navigation" class="govuk-header__navigation-list"> <li class="govuk-header__navigation-item"> <a class="govuk-header__link" href="/about"> About </a> </li><li class="govuk-header__navigation-item"> <a class="govuk-header__link" href="/tools"> Tools </a> </li><li class="govuk-header__navigation-item"> <a class="govuk-header__link" href="/prompts"> Prompts </a> </li><li class="govuk-header__navigation-item"> <a class="govuk-header__link" href="/how-tos"> How-tos </a> </li> </ul> </nav> </div> </div> </div>  </header>  <div class="govuk-width-container"> <main id="main" class="govuk-main-wrapper govuk-!-padding-bottom-9 govuk-!-padding-top-7"> <nav class="govuk-breadcrumbs govuk-!-margin-top-0 govuk-!-margin-bottom-5" aria-label="Breadcrumb"> <ol class="govuk-breadcrumbs__list"> <li class="govuk-breadcrumbs__list-item"> <a class="govuk-breadcrumbs__link govuk-!-margin-0" href="/">Home</a> </li> <li class="govuk-breadcrumbs__list-item"> <a class="govuk-breadcrumbs__link govuk-!-margin-0" href="/how-tos">How-tos and resources</a> </li> </ol> </nav> <div class="govuk-grid-row"> <div class="govuk-grid-column-two-thirds govuk-!-margin-bottom-7 govuk-!-padding-bottom-5" data-pagefind-body> <h1 class="govuk-heading-xl">Security and risks (to be updated)</h1> <p class="govuk-body">
This information complements and updates the
<a href="https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government" class="govuk-link">
AI Playbook for the UK Government
</a>
. It reflects the latest guidance and best practice. This will be updated as needed.</p> </div> </div> <div class="govuk-grid-row"> <div class="govuk-grid-column-one-third kh-sticky-sidebar kh-sticky-sidebar--with-back-to-top"> <on-this-page sidebar="true"></on-this-page> <div class="kh-back-to-top" data-astro-cid-qrl4wijt> <a class="govuk-link govuk-link--no-visited-state" href="#top" data-astro-cid-qrl4wijt> <svg role="presentation" focusable="false" class="kh-back-to-top__icon" xmlns="http://www.w3.org/2000/svg" width="13" height="17" viewBox="0 0 13 17" data-astro-cid-qrl4wijt> <path fill="currentColor" d="M6.5 0L0 6.5 1.4 8l4-4v12.7h2V4l4.3 4L13 6.4z" data-astro-cid-qrl4wijt></path> </svg>Back to top
</a> </div>  </div> <div class="govuk-grid-column-two-thirds kh-guidance--draft" id="content"><p class="govuk-body">Cyber security is a primary concern for all government services, as laid out in the <a class="govuk-link" href="https://www.gov.uk/government/publications/government-cyber-security-strategy-2022-to-2030">Government Cyber Security Strategy</a>. When building and deploying new services, including AI systems, the government has a responsibility to make sure these are secure to use and resilient to cyber attacks. To meet this requirement, your service must comply with the government’s <a class="govuk-link" href="https://www.security.gov.uk/guidance/secure-by-design/">Secure by Design</a> principles before it can be deployed.</p>
<p class="govuk-body">Developers and deployers of AI should follow the <a class="govuk-link" href="https://eur02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.etsi.org%2Fnewsroom%2Fpress-releases%2F2521-etsi-technical-specification-sets-international-benchmark-for-securing-artificial-intelligence&data=05%7C02%7CTommaso.Spinelli%40dsit.gov.uk%7C998ecd173473425f648508ddbd451cf1%7Ccbac700502c143ebb497e6492d1b2dd8%7C0%7C0%7C638874826748172884%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=VRMG70GDVv3p7xHbcJu8skphiGxdsevlP2CpqLHKIQA%3D&reserved=0">European Telecommunication Standards Institute’s (ETSI) Technical Specification</a>, which has been developed with DSIT and the NCSC to be a baseline global Cyber Security standard for AI models and systems. You should also look at the <a class="govuk-link" href="https://eur02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fwww.etsi.org%2Fdeliver%2Fetsi_tr%2F104100_104199%2F104128%2F01.01.01_60%2Ftr_104128v010101p.pdf&data=05%7C02%7CTommaso.Spinelli%40dsit.gov.uk%7C998ecd173473425f648508ddbd451cf1%7Ccbac700502c143ebb497e6492d1b2dd8%7C0%7C0%7C638874826748196528%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=tNOEqhPPJIzp66G8SsKq3bR7F4WYajLCW97X5UIYni4%3D&reserved=0">Securing Artificial Intelligence (SAI); Guide to Cyber Security for AI Models and Systems</a>, which is an accompanying implementation guide with use cases.</p>
<p class="govuk-body">There are some security risks that apply uniquely to AI and generative AI technologies. This section takes you through some of these risks.</p>
<p class="govuk-body">You can learn more about AI security at the cross-government AI security group, which brings together security practitioners, data scientists and AI experts. You must have a gov.uk email address and to join, contact <a class="govuk-link" href="mailto:x-gov-genai-security-group@digital.cabinet-office.gov.uk">x-gov-genai-security-group@digital.cabinet-office.gov.uk</a>.</p>
<h3 class="govuk-heading-s" id="publicaiapplicationsandwebservices">Public AI applications and web services</h3>
<p class="govuk-body">A simple way to implement an AI solution is to use publicly available commercial applications, such as Google Gemini or ChatGPT in the case of generative AI. While you might think that these public tools are more secure, you should consider that you cannot easily control the data input to the models. You must rely on educating users on what data they can and cannot enter into these services.</p>
<p class="govuk-body">You also have no control over the outputs from these models, and you’re subject to their commercial licence agreements and privacy statements. For example, <a class="govuk-link" href="https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance">OpenAI</a> states in its article about <a class="govuk-link" href="https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance">how your data is used to improve model performance</a> that it uses the prompt data you enter directly into the ChatGPT website to improve their models (although individual users can opt out). For further guidance on what information you can safely enter when using public AI applications see <a class="govuk-link" href="#usingaiwithgovernmentinformation">using AI with government information</a>.</p>
<h3 class="govuk-heading-s" id="embeddedaiapplications">Embedded AI applications</h3>
<p class="govuk-body">Many vendors include AI features and capabilities directly within their products, including Slack GPT and Microsoft Copilot. While this guidance applies at a high level to each of these applications, they come with their own unique security concerns. Before adopting any of these products it’s important to understand the underlying architecture of the solution, and what mitigations the vendor has put in place for the inherent risks associated with AI.</p>
<p class="govuk-body">In addition to embedded applications, there are also many AI-powered plugins or extensions to other software. For example, Visual Studio Code has a large ecosystem of community-built extensions, many of which offer AI functionality. You must take extreme caution before installing any unverified extensions as these can pose a security risk.</p>
<p class="govuk-body">There has also been a proliferation of AI transcription tools that are capable of joining virtual meetings and transcribing meeting notes. These present a serious risk of data leakage as they silently upload meeting recordings to an AI service for transcription and analysis. When hosting virtual meetings, organisers should verify the identity of all attendees and state up front that the use of third-party meeting transcription tools is not allowed.</p>
<p class="govuk-body">You should always speak with your security team to discuss your requirements before deploying any embedded AI applications, extensions or plugins.</p>
<h3 class="govuk-heading-s" id="publicaiapis">Public AI APIs</h3>
<p class="govuk-body">Many public AI applications offer the ability to access their services through APIs. By using the API you can integrate AI capabilities into your own applications, intercept the data being sent to the AI model, and also process the responses before returning them to the user.</p>
<p class="govuk-body">For example, when integrating an LLM through an API you can include privacy-enhancing technology to prevent data leakage, add content filters to sanitise the prompts and responses, and log and audit all interactions with the model. Be aware that PETs come with their own limitations. Therefore, PET selection should be proportionate to the sensitivity of the data.</p>
<p class="govuk-body">Refer to the ICO's guidance on <a class="govuk-link" href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/data-sharing/privacy-enhancing-technologies/">privacy-enhancing technologies (PETs)</a> and the RTA’s <a class="govuk-link" href="https://www.gov.uk/government/publications/privacy-enhancing-technologies-adoption-guide">PET adoption guide</a> for more information. Consider also that your data is still passed over to the provider when you use an API, although retention policies tend to be more flexible for API use. OpenAI, as an example, states in its <a class="govuk-link" href="https://openai.com/enterprise-privacy/#api-platform-faq">enterprise privacy at OpenAI</a> statement that it only <a class="govuk-link" href="https://openai.com/enterprise-privacy#api-platform-faq">retains prompt data sent to the API for 30 days</a>.</p>
<h3 class="govuk-heading-s" id="privatelyhostedaimodels">Privately hosted AI models</h3>
<p class="govuk-body">Instead of using a public AI offering, the alternative is to host your own AI model. This could be a model taken from one of the many publicly available pre-trained open source models or it could be a model you have built and trained yourself. By running a model in your own private cloud infrastructure, you ensure that data never leaves an environment that you own.</p>
<p class="govuk-body">In the case of generative AI, the open models you can run in this way are increasingly comparable to the publicly available ones, with acceptable results for many applications. The advantage is that you have complete control over the model and the data it consumes. The disadvantage is that you're responsible for ensuring the model is secure and up to date. Consider that you must also maintain the infrastructure to host your model, which brings additional costs along with the specialist skills you’ll need in ML operations.</p>
<h3 class="govuk-heading-s" id="managedmachinelearningmodelhostingplatform">Managed machine learning model hosting platform</h3>
<p class="govuk-body">An alternative approach to setting up the infrastructure to host your own model from scratch is to use a fully managed ML model hosting platform. For example, <a class="govuk-link" href="https://aws.amazon.com/bedrock/">Amazon Bedrock</a> and <a class="govuk-link" href="http://watsonx.ai">IBM watsonx.ai</a> allow you to host different open source or commercially available AI models like <a class="govuk-link" href="https://www.llama.com/">Meta Llama</a> or <a class="govuk-link" href="https://claude.ai/login?returnTo=%2F%3F">Anthropic Claude</a>, and to compare their performance. <a class="govuk-link" href="https://learn.microsoft.com/en-gb/azure/ai-services/openai/overview">Microsoft Azure OpenAI service</a> offers access to the OpenAI GPT but as models running in a private instance with zero-day retention policies.</p>
<h3 class="govuk-heading-s" id="runningaimodelslocally">Running AI models locally</h3>
<p class="govuk-body">Many open source AI models are capable of being run locally on a single machine. This is often attractive because it allows the models to be run in isolation, with limited or no network access. This type of deployment is not recommended for most production services, but might be appropriate for solutions which need access to highly confidential data which cannot be used in the public or commercial cloud.</p>
<h3 class="govuk-heading-s" id="trainingaimodels">Training AI models</h3>
<p class="govuk-body">In addition to where your AI model runs, you should also consider how it was trained from a security perspective. Many of the publicly available generative AI models were trained using data from the public internet. This means that they could include data that is personally identifiable, inaccurate, illegal or harmful, and any of these could present a security risk.</p>
<p class="govuk-body">It’s possible to train an AI model using your own data, and for many specific and limited tasks this is often the most appropriate approach because it gives you complete control. For generative AI models, the cost of doing this for larger and more capable systems is prohibitive and the amount of private data required to produce acceptable performance of a large model is beyond the capacity of most organisations. You should assume that any data you use to train your model could be extracted by an attacker. There’s more about this in the <a class="govuk-link" href="#dataleakage">data leakage</a> section.</p>
<h3 class="govuk-heading-s" id="workingwithyourorganisationaldata">Working with your organisational data</h3>
<p class="govuk-body">A key application of AI is working with your organisation's private data. By enabling the model to access, understand and use this data, insights and knowledge can be provided to users that are specific to their subject domain and will provide more reliable results. Standard ML models can be trained directly with your private data set. Generative AI models can be fine-tuned, or you can use approaches like retrieval augmented generation (RAG) to augment the model with your private data.</p>
<p class="govuk-body">If you use your own data with an AI model, you immediately increase the data security risk and will need to apply additional security controls to stop data leakage and privacy violations.</p>
<p class="govuk-body">When using your own private data with an AI model, you should consider:</p>
<ul class="govuk-list govuk-list--bullet govuk-list--spaced">
<li><p class="govuk-body">where is your data being sent and how it is being processed</p></li>
<li><p class="govuk-body">if your data is being used to train future models</p></li>
<li><p class="govuk-body">how long your data is being retained</p></li>
<li><p class="govuk-body">if your data is being logged, who has access to those logs and for what purpose</p></li>
</ul>
<h3 class="govuk-heading-s" id="opensourcevsclosedsourcemodels">Open source vs closed source models</h3>
<p class="govuk-body">Open source and closed source AI models are generally equally secure.</p>
<p class="govuk-body">Closed source models are typically proprietary, so they are developed and maintained by a specific company or organisation such as OpenAI’s GPT 4 or Google’s Gemini. These models often have limited transparency regarding their inner workings, training data, and code. Access to these models are usually governed by specific licensing agreements.</p>
<p class="govuk-body">Open source models typically make their model parameter weights publicly available, as in Meta’s Llama models. Open weights allow the model to be run locally, giving more control over data flow. Some models also open their training data and the code used to train the model. One example is the <a class="govuk-link" href="https://allenai.org/olmo">Allen Institute for AI’s OLMo 2 models</a>.</p>
<p class="govuk-body">Open source models are released under different licenses, with some being more permissive than others. Open source models can provide greater opportunities for analysis by contributing to enhanced transparency and responsible AI practices. Providers of open weight models actively work to detect, track, and mitigate security issues. For example, Meta provides open source tools such as <a class="govuk-link" href="https://ai.meta.com/research/publications/purple-llama-cyberseceval-a-benchmark-for-evaluating-the-cybersecurity-risks-of-large-language-models/">CyberSecEval</a> and <a class="govuk-link" href="https://www.llama.com/docs/model-cards-and-prompt-formats/prompt-guard/">PromptGuard</a>, as well as carrying out adversarial red teaming.</p>
<p class="govuk-body">On-premise hosting of open models is becoming increasingly important in use cases handling sensitive data such as healthcare and national security, as the data flow can be tightly controlled. But running an open weight model on-premises does not guarantee security. If the model is from a less trusted source, there is still the risk of model poisoning.</p>
<h2 class="govuk-heading-m" id="aisecurityrisks">AI security risks</h2>
<p class="govuk-body">AI security risks come in two categories - the risks of using AI, and the risks of adversaries using AI against you.</p>
<h3 class="govuk-heading-s" id="usingai">Using AI</h3>
<p class="govuk-body">There are many resources you can use to explore the security risks of using AI. These include:</p>
<ul class="govuk-list govuk-list--bullet govuk-list--spaced">
<li><p class="govuk-body">a set of <a class="govuk-link" href="https://www.ncsc.gov.uk/collection/machine-learning">principles for security of machine learning (ML) solutions</a>, published by the NCSC</p></li>
<li><p class="govuk-body">a list of <a class="govuk-link" href="https://learn.microsoft.com/en-us/security/engineering/failure-modes-in-machine-learning">failure modes in ML</a>, compiled by Microsoft</p></li>
<li><p class="govuk-body">the <a class="govuk-link" href="https://atlas.mitre.org/matrices/ATLAS">MITRE Adversarial Threat Landscape for AI Systems (ATLAS) matrix</a>, which is an open source knowledge base of techniques used to attack AI systems</p></li>
<li><p class="govuk-body">the <a class="govuk-link" href="https://www.gov.uk/government/publications/international-scientific-report-on-the-safety-of-advanced-ai">international scientific report on the safety of advanced AI</a><a class="govuk-link" href="https://www.gov.uk/government/publications/international-scientific-report-on-the-safety-of-advanced-ai">, which is an analysis of risks posed by general purpose advanced AI systems</a></p></li>
<li><p class="govuk-body">significant work by the Open Worldwide Application Security Project (OWASP) to identify the <a class="govuk-link" href="https://owasp.org/www-project-top-10-for-large-language-model-applications/">top 10 security risks for LLMs</a> . These risks focus on the use of LLMs but many of them will also apply to other types of generative AI models and more widely to AI in general</p></li>
</ul>
<p class="govuk-body">From a combination of these sources, we can draw out some of the most common vulnerabilities and discuss them in context of AI applications in government.</p>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="dataandmodelpoisoning">Data and model poisoning</h4>
<p class="govuk-body">Data or model poisoning is when data used to train an AI model has been tampered with, leading the model to produce incorrect or harmful output.</p>
<p class="govuk-body">Attackers can target the data used to train an AI model to introduce vulnerabilities, backdoors or biases that compromise the model’s security and behaviour.</p>
<p class="govuk-body">For a traditional ML model that uses a limited amount of training data for a specific task, this type of attack can be prevented by training the model yourself on a known data set that you control.</p>
<p class="govuk-body">For frontier generative AI models, the barrier to entry for training a model from scratch is high, and fine-tuning an existing model is much easier and cheaper. There are many open source models that are easy to fine-tune (for image generation, for example), and these can and are used to produce specific types of outputs, some of which are harmful or illegal.</p>
<p class="govuk-body">When using an AI model, particularly a specialised, fine-tuned model, from a third-party source, it’s difficult to ascertain if it has been tampered with. Poisoned models may appear to be functioning as expected until a specific prompt triggers the malicious behaviour.</p>
<p class="govuk-body">Supply chain vulnerabilities of this kind are not unique to AI. When software libraries are hacked, all downstream systems that depend on those libraries are affected. A notable example of this was the Faker NPM hack, and more information about this can be found in the article <a class="govuk-link" href="https://www.bleepingcomputer.com/news/security/dev-corrupts-npm-libs-colors-and-faker-breaking-thousands-of-apps/">dev corrupts NPM libs 'colors' and 'faker' breaking thousands of apps</a>.</p>
<p class="govuk-body">Many automated tools exist for detecting, tracking and fixing security issues with open source software, including a growing number of tools designed to be used with open source AI models. The popular open source AI model site <a class="govuk-link" href="https://huggingface.co/">Hugging Face</a> has <a class="govuk-link" href="https://huggingface.co/docs/hub/security-malware">malware scanning tooling</a>, but this is not yet capable of determining if a given AI model has been trained on poisoned data.</p>
<p class="govuk-body">AI model hosting platforms like Microsoft’s <a class="govuk-link" href="https://azure.microsoft.com/en-gb/products/ai-studio/">Azure AI</a>, Amazon’s <a class="govuk-link" href="https://aws.amazon.com/bedrock/">Bedrock</a> and IBM’s <a class="govuk-link" href="http://watsonx.ai">watsonx.ai</a> allow developers to use commercial models and other third-party AI models. These services do not make any guarantees about the security and integrity of the third-party models that they’re capable of hosting.</p>
<p class="govuk-body">Training data can also be poisoned indirectly through the introduction of malicious data into known collections of open data that are used to train or fine-tune frontier generative AI models. This is likely to become an increasing threat as hackers learn which publicly available data sets, such as those from Wikipedia or Reddit, have been used to train generative AI models, and target these to poison future versions of the models.</p>
<p class="govuk-body">The impacts of an AI model trained or fine-tuned with poisoned data are wide ranging, including direct security threats to the organisation running the model and biased or harmful outputs to the users of the model. Poisoned models could push people to particular products or subvert confidence in government services.</p>
<p class="govuk-body">To help detect and prevent data poisoning, you’ll need to make sure your users and developers are trained on the risks and aware that the results from AI models can be false or biased. Outputs should be tested against known good responses and be systematically tested for biases.</p>
<p class="govuk-body">ML hosting platforms often include evaluation tools that can measure and test the performance of an ML model, such as <a class="govuk-link" href="https://cloud.google.com/vertex-ai/docs/evaluation/introduction">Google’s Vertex AI</a> or <a class="govuk-link" href="https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/concept-model-monitoring-generative-ai-evaluation-metrics?view=azureml-api-2">Microsoft’s Prompt Flow</a>. Improved explainability of the models themselves&nbsp; also help, enabling the output to be traced back to the source training data. For more information, refer to the <a class="govuk-link" href="#transparencyandexplainability">transparency and explainability</a> section.</p>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="dataleakage">Data leakage</h4>
<p class="govuk-body">Data leakage is when responses from an AI model reveal confidential information such as personal data.</p>
<p class="govuk-body">AI models are trained using data. In the case of generative AI, data is commonly taken from the public internet and will contain personal data and other confidential information. AI models can suffer from data leakage, depending on their intended mode of operation. For example, a model that is being used as a classifier to rank or sort data into groups based on criteria would necessarily be less likely to leak training data than a generative AI model as its outputs are confined to the specific classification problem. However, if an AI model has been trained or fine-tuned with private data that has different levels of security controls based on the user who should be seeing it, there is currently no way to preserve these controls when training the model.</p>
<p class="govuk-body">Cornell University’s study on <a class="govuk-link" href="https://arxiv.org/abs/2311.17035">scalable extraction of training data from (production) language models</a> shows how generative AI models can also be made to reveal their original training data through their responses, meaning that any outputs from a generative AI model could potentially contain confidential information. For generative AI, a way to preserve user access controls is to use in-context learning. This is where a search is carried out first on the private data a user is permitted to see, and the retrieved results are passed in context to the generative AI model. This type of approach is known as retrieval augmented generation (RAG), and is used in many commercial generative AI tools like Microsoft Copilot.</p>
<p class="govuk-body">In-context learning has limitations and can degrade the performance of a generative AI model in certain applications. RAG tools are susceptible to indirect prompt injection, either in the information retrieved by the initial search or through the user prompt, meaning that security controls could be circumvented and private data could still be leaked.</p>
<p class="govuk-body">You should make sure your AI model only has access to the data the user of the model should be able to access.</p>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="insecureaitoolchain">Insecure AI tool chain</h4>
<p class="govuk-body">An insecure tool chain is when tools used to train, fine-tune, host, serialise and operate AI models are not secure.</p>
<p class="govuk-body">Specialised tools built to support AI models have been found to lack basic security features. For example, the <a class="govuk-link" href="https://huggingface.co/docs/hub/security-pickle">Pickle format</a> used to serialise ML models has serious security flaws. This may be because the tools were developed at pace by AI researchers and data scientists who were not following secure coding practices. AI tools often have elevated access rights to the systems they’re running on, making the impact of a security breach even worse. This is not a new risk, as any developer tooling can be insecure, but AI tools appear to be particularly prone to security issues. It’s easier for a hacker to target the tool chain for an AI model than the model itself.</p>
<p class="govuk-body">You should make sure your cybersecurity team has approved the tools you use to support your AI models. This includes checking that the tools implement user authentication and follow the principle of least privilege, meaning they are not running with administrator permissions to your system.</p>
<p class="govuk-body">An example of this would be using an alternative to Pickle for model serialisation. There are more secure model serialization formats such as <a class="govuk-link" href="https://huggingface.co/docs/transformers/en/gguf">GGUF</a>, <a class="govuk-link" href="https://huggingface.co/blog/introduction-to-ggml">GGML</a>, or <a class="govuk-link" href="https://huggingface.co/docs/safetensors/index">Safetensors</a>, and you could embed this requirement in your departmental coding standards.</p>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="exacerbatespreviouslyexistingrisks">Exacerbates previously existing risks</h4>
<p class="govuk-body">Previously existing risks can be exacerbated, such as poor data management, insufficient security classification, insecure storage of credentials, and more.</p>
<p class="govuk-body">An example of this sort of risk is over-privileged access. This happens when an AI tool is used to enhance enterprise-wide search capabilities. A user may not be aware of sensitive data that they currently have access to on a government system, but when they use an AI-enhanced search tool, the power of the tool exposes the lack of access controls and brings back sensitive data that the user was unaware of being able to see. The AI tool is not creating this issue as the problem already exists, but the AI tool is making it worse.</p>
<p class="govuk-body">In line with the advice of vendors, you should review all enterprise access controls before deploying an AI tool to your system. This should be an ongoing exercise because no system is static. It’s essential that you’re able to continually monitor and review access controls when deploying AI applications across your organisation.</p>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="perturbationattack">Perturbation attack</h4>
<p class="govuk-body">A perturbation attack is when an attacker stealthily modifies the inputs to an AI model to get a desired response.</p>
<p class="govuk-body">An example of this type of threat is a CV system for medical diagnostics trained to distinguish between abnormal and normal scans. The system can be fooled when presented with an image containing specific amounts of noise, causing it to classify a scan incorrectly. Mitigations include adversarial training of the model with noisy images to improve robustness against this type of attack.</p>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="promptinjection">Prompt injection</h4>
<p class="govuk-body">Prompt injection is when hackers use prompts that can make the generative AI model behave in unexpected ways.</p>
<p class="govuk-body">Prompt injection is a type of perturbation attack specifically targeted at generative AI systems that use text prompts to generate new content, including text, image, audio and video. Developers lay down rules about how a model should behave and respond as system instructions, which are provided to the model along with the user prompt. Fundamentally, a generative AI model cannot distinguish between the user prompt and these system instructions because both are just seen as input to the model. A hacker can exploit this flaw by crafting special prompts that circumvent the system instructions, causing the model to respond in an unintended way.</p>
<p class="govuk-body">The potential impact of prompt injections ranges from very mild,&nbsp;like a user making a banking chatbot tell jokes in the style of a pirate,&nbsp;to much more serious. For example, a hacker might trick a generative AI model designed to send alerts to patients about medical appointments into sending fake messages about non-existing appointments.</p>
<p class="govuk-body">Prompt injections come in 2 forms, which are:</p>
<ol class="govuk-list govuk-list--number govuk-list--spaced">
<li>direct, when the user who is interacting with the generative AI model crafts the prompt injection themselves</li>
<li>indirect, when other information that is being sent to a generative AI model is tampered with to include a prompt injection. For example, an email attachment can include a prompt and when a generative AI model that is tasked with summarising emails reads the attachment, the prompt injection is triggered</li>
</ol>
<p class="govuk-body">Generative AI has the ability to take natural language inputs and have a machine act on them. A common pattern for using LLMs in this way is called ReAct (or <a class="govuk-link" href="https://arxiv.org/abs/2210.03629">Reason-Act</a>). Here the LLM is prompted to reason about how to perform a task, and the response from the model is processed and used to automate calls to different services that perform actions. Hackers can subvert this approach to make the model perform different actions, which significantly limits the utility of generative AI in fully automated solutions. To make sure the model is doing the right thing, there must be a human present to review the action before carrying it out. This is why the majority of commercial applications of generative AI are in the space of human assistants and are known as copilots.</p>
<p class="govuk-body">Work is underway to address the prompt injection issue and a number of mitigations are already available. These include:</p>
<ul class="govuk-list govuk-list--bullet govuk-list--spaced">
<li><p class="govuk-body">filtering prompts before they’re sent to the model by sending the prompts through another ML model trained to detect likely prompt injections</p></li>
<li><p class="govuk-body">filtering the outputs of the model before they’re returned to the user</p></li>
<li><p class="govuk-body">more speculative work around fine-tuning models to better distinguish between user input and system prompts. An example of this is <a class="govuk-link" href="https://arxiv.org/html/2312.14197v1">benchmarking and defending against indirect prompt injection attacks on large language models</a></p></li>
</ul>
<p class="govuk-body">To defend against prompt injection, you should log all prompts sent to a model and carry out ongoing audits to determine if prompt injection is happening. You must also block users you find who are responsible.</p>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="hallucinations">Hallucinations</h4>
<p class="govuk-body">Hallucinations are when the generative AI model responds with information that appears to be truthful but is actually false.</p>
<p class="govuk-body">Counterintuitively for a machine, generative AI is better at creative tasks than fact retrieval. This is because all generative AI models predict and generate content by determining the most likely subsequent pattern based on previous training, such as an LLM predicting the next most likely word. The models are therefore very good at generating plausible predictions that look correct but may not actually be correct. The risk is that overreliance by human operators on the outputs of generative AI models results in misinformation, miscommunication, legal issues and security vulnerabilities.</p>
<p class="govuk-body">Fundamentally, generative AI models cannot be trusted to produce factual content. Any generative AI services that output generated content directly to the public, such as an LLM-powered chatbot giving advice on a government website, would be prone to hallucination and could lead to someone being misled about a government service, policy or point of law.</p>
<p class="govuk-body">A <a class="govuk-link" href="https://www.bbc.com/travel/article/20240222-air-canada-chatbot-misinformation-what-travellers-should-know">legal case in Canada</a> found an organisation that owned a site with a hallucinating chatbot financially responsible for the bad advice it dispensed. In the worst case, hallucination could even lead to direct harm if a user acted on faulty advice, such as a user being advised not to seek medical attention when they needed to.</p>
<p class="govuk-body">In addition to this direct risk, there’s also a significant indirect risk if officials are relying on generative AI as a primary information source when providing the public with guidance, advising ministers or informing policy decisions.</p>
<p class="govuk-body">You should make sure your users are trained to critically assess the outputs of generative AI or not to rely exclusively on their responses. Specifically around cybersecurity, if security practitioners in government become overly reliant on the advice of generative AI assistants, they may become less effective at spotting novel attacks and may even be misled into following bad advice and exposing systems to increased cybersecurity risks.</p>
<h3 class="govuk-heading-s" id="adversariesusingai">Adversaries using AI</h3>
<p class="govuk-body">The following risks are those posed by other, potentially malicious actors who attempt to subvert the functionality of AI systems.</p>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="misinformation">Misinformation</h4>
<p class="govuk-body">Misinformation is when AI is used to create realistic synthetic media, leading to the spread of misinformation, undermining trust in digital media and manipulating public opinion.</p>
<p class="govuk-body">Adversaries could use AI to interfere in electoral processes and spread misinformation. What makes this threat more potent is the ease with which bad actors can produce content for multiple audiences in many languages, with translations reflecting nuance and common parlance to make them more credible.</p>
<p class="govuk-body">The technical ease with which generative AI models can be integrated with social media or other platforms also means that bad actors could spread misinformation automatically and at scale. At present, misinformation is the most pressing risk that AI (particularly generative AI) presents to governments, specifically relating to the integrity of democratic elections. There have already been a number of instances of suspected AI-generated fake media being deployed in the US, such as <a class="govuk-link" href="https://www.bbc.co.uk/news/world-us-canada-68064247">fake Biden robocalls</a>. With the release of new, more powerful generative AI models capable of generating realistic video content, such as OpenAI’s <a class="govuk-link" href="https://openai.com/sora">video generation model Sora</a>, this risk is only going to increase.</p>
<p class="govuk-body">Big tech companies have committed to address the issue by including <a class="govuk-link" href="https://arstechnica.com/ai/2023/07/openai-google-will-watermark-ai-generated-content-to-hinder-deepfakes-misinfo/">watermarks in the generated AI content</a> their models create, but so far this has not become widespread. <a class="govuk-link" href="https://www.bbc.co.uk/news/technology-66618852">Google has announced a tool</a> that can detect and watermark AI-generated content. The <a class="govuk-link" href="https://c2pa.org/">C2PA open standard</a> for embedding metadata into media content, which allows the source and provenance of the content to be verified, is also gaining some traction.</p>
<p class="govuk-body">When building services that receive and process digital content like text, images or even video, you’ll need to consider the impact of that content being generated by AI and therefore being unreliable, misleading or malicious. For more information about the ethical implications of misinformation, refer to the societal wellbeing and public good section.</p>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="phishing">Phishing</h4>
<p class="govuk-body">Phishing is when generative AI is used to craft more convincing phishing emails and messages that can be tailored to specific user groups, leading to an increase in internet fraud.</p>
<p class="govuk-body">LLMs make it easy for fraudsters to create convincing phishing emails and messages in different languages, even those they do not speak. LLMs can be easily automated to produce unique, targeted and personalised phishing emails at scale, making detection much harder. There is already some evidence that the amount of phishing emails and messages is rising, with the likely cause being the advent of generative AI, as reported in the article <a class="govuk-link" href="https://www.prnewswire.com/news-releases/fido-alliance-study-reveals-growing-demand-for-password-alternatives-as-ai-fuelled-phishing-attacks-rise-301957007.html">FIDO Alliance study reveals growing demand for password alternatives as AI-fuelled phishing attacks rise</a>.</p>
<p class="govuk-body">The government is likely to see an increase in phishing emails and social engineering attacks as a result of generative AI. The risk of cyber security breaches through targeted, socially engineered attacks driven by generative AI could become more acute, as it may become easier to identify likely targets by using generative AI to trawl across social networks and other public resources, looking for contact details for government employees in sensitive roles.</p>
<p class="govuk-body">To detect scams of this sort, you’ll need more sophisticated counter measures. For example, you could use another specially trained ML model to detect and block phishing emails produced by generative AI.</p>
<p class="govuk-body">You’ll also need to educate users about how to detect AI-produced fake messages, because previous red flags such as badly formed sentences and incorrect spelling will no longer be enough. The likelihood is that phishing attacks will become more targeted, and use more sophisticated social engineering techniques to gain the recipient’s trust.</p>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="cyberattacks">Cyber attacks</h4>
<p class="govuk-body">Cyber attacks can happen when generative AI lowers the bar for entry for hackers to create malware and craft cyber attacks that are more sophisticated and harder to detect.</p>
<p class="govuk-body">Generative AI has proved to be highly capable at aiding developers in writing effective code. This is because the AI model provides the developer with the majority of the solution, including prerequisites and boilerplate code, leaving the developer only needing to finesse the final details. A hacker using a generative AI model specifically to create malware or craft a cyber attack is likely to more quickly and easily achieve a working attack.</p>
<p class="govuk-body">All large commercial generative AI models have filters in place to try to detect if a user is asking the model to create malware. However, these filters can be subverted. For more information, see <a class="govuk-link" href="#promptinjection">prompt injection threats</a>.</p>
<p class="govuk-body">The expectation from some cyber security experts is that the number and sophistication of cyber attacks are likely to rise due to the use of generative AI. This is because more bad actors who previously were excluded from being able to create credible threats are now able to do so.</p>
<p class="govuk-body">The unique position of government, and the capabilities and desire of hostile state-sponsored groups, mean that this threat is likely to be a key concern for government cybersecurity teams. The potential for escalating levels of sophisticated cyber attacks fuelled by generative AI is real, although <a class="govuk-link" href="https://www.microsoft.com/en-us/security/blog/2024/02/14/staying-ahead-of-threat-actors-in-the-age-of-ai/">research by Microsoft and OpenAI</a> has yet to observe any particularly novel or unique attacks resulting from the use of AI. The area is under constant review.</p>
<p class="govuk-body">You should expect increased numbers of cyber attacks and take steps to increase your existing cyber security defences. For more information, refer to the NCSC report on the <a class="govuk-link" href="https://www.ncsc.gov.uk/report/impact-of-ai-on-cyber-threat">near-term impact of AI on the cyber threat</a>.</p>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="fakeofficialcorrespondence">Fake official correspondence</h4>
<p class="govuk-body">Fake official correspondence is when generative AI is used to craft convincingly human correspondence which can either be automated and sent at scale to organisations, flooding their usual communications channels, or lead to unfair outcomes when judged against human correspondence.</p>
<p class="govuk-body">An example of this kind of threat might be a hacker using generative AI to create thousands of requests for information from a government department, seemingly sent from multiple unique people. Similar to the phishing threat, the ability of LLMs to create convincing and plausible text in an automated way, at scale, makes this type of attack particularly concerning. A hacker could overwhelm an organisation’s normal communications channels, causing an organisation to spend time and money responding to seemingly genuine requests while degrading their ability to cope with real people.</p>
<p class="govuk-body">Another example of this sort of threat, at a lower scale, is a fraudster submitting official information that will be used to judge a decision, and where the use of generative AI to create fake answers may prejudice the decision.</p>
<p class="govuk-body">Areas of particular concern for the UK government are commercial procurement, recruitment, freedom of information requests, and the processing of claims that require an evidence-based decision.</p>
<p class="govuk-body">Mitigations are similar to those for phishing or misinformation attacks, such as processing all correspondence through another ML model trained to detect AI-generated content. When running services that result in decisions based on evidence provided through official correspondence, you should consider the potential impact on the service of the correspondence being AI-generated.</p>
<h3 class="govuk-heading-s" id="securityopportunities">Security opportunities</h3>
<p class="govuk-body">In addition to the threats posed by AI, there are also opportunities to improve cyber security through the use of AI. These are:</p>
<ul class="govuk-list govuk-list--bullet govuk-list--spaced">
<li><p class="govuk-body">threat detection, where AI can be used to improve threat detection systems by generating synthetic cyber attack data for training more robust models, or by directly detecting anomalies in real-time cyber security data. AI models can also be used to analyse historical cyber security data and identify patterns associated with known threats. These patterns can then be used to detect anomalies in real-time network traffic or system behaviour. When unusual activity occurs, the AI model can trigger an alert to be raised to human operators.</p></li>
<li><p class="govuk-body">incident response, where AI models which are trained or fine-tuned on large amounts of historical cyber security data can predict future threats. By recognising subtle changes in patterns, they can be used to anticipate emerging attack vectors. Generative AI can assist in incident response by automating the generation of reports, recommending remediation actions based on past data, or filtering out noise in verbose cyber security logs, allowing human analysts to focus on the most important information.</p></li>
<li><p class="govuk-body">security testing, where generative AI can create security test cases and improve the efficiency and coverage of security testing. Instead of manually crafting test scenarios, security professionals can use generative AI models to mimic adversary behaviour, simulate various attacks, and analyse existing vulnerabilities, attack patterns and system behaviours. LLMs are good at analysing code, meaning they can also be used to review source code, point out security flaws, and generate secure code snippets based on security best practices.</p></li>
<li><p class="govuk-body">enhancing vulnerability management, where generative AI can assist in documenting security products. LLMs can be used to process the large amounts of documentation, guidance and online help around different security tools and their features and limitations, providing summarised information and enhanced search capabilities. Internet-enabled LLMs can also provide up-to-date insights, helping prioritise vulnerability patches and updates.</p></li>
</ul>
<h3 class="govuk-heading-s" id="scenarios">Scenarios</h3>
<p class="govuk-body">The scenarios discussed below build on the security risks identified in this section, and will help you to understand how they apply to some of the applications of AI in government.</p>
<p class="govuk-body">Each scenario includes descriptions of potential impacts and mitigations. The likelihood and impact of each risk is scored following the approach outlined in the <a class="govuk-link" href="https://owasp.org/www-community/OWASP_Risk_Rating_Methodology">OWASP risk rating methodology</a>. In addition to the impact factors included in the OWASP approach, user harm and misinformation are discussed as significant impact factors.</p>
<p class="govuk-body">This list of security threat scenarios is not exhaustive, but you can use the scenarios as a template for assessing the risks associated with different applications of AI.</p>
<p class="govuk-body">These scenarios include:</p>
<ul class="govuk-list govuk-list--bullet govuk-list--spaced">
<li><p class="govuk-body">perturbation attacks, where an attacker stealthily modifies the inputs to an AI model to get a desired response. Consider mitigations such as <a class="govuk-link" href="#scenario">identity verification using image and video capture technology</a></p></li>
<li><p class="govuk-body">insecure AI tool chains, where tools used to train, fine-tune, host, serialise and operate AI models are not secure. For mitigations, see <a class="govuk-link" href="#toolsusedwithdefaultconfiguration">machine learning operations (MLOps) tools used with default configuration</a></p></li>
<li><p class="govuk-body">prompt injection threats, which use prompts that can make the generative AI model behave in unexpected ways. For mitigations, see <a class="govuk-link" href="#scenario">LLM chatbot on a government website</a></p></li>
<li><p class="govuk-body">data leakage, where responses from the LLM reveal sensitive information such as personal data. For mitigations, see <a class="govuk-link" href="#enterpriseAIsearchtoolsummarisingemails">enterprise AI search tool summarising emails</a></p></li>
<li><p class="govuk-body">hallucinations, where the LLM responds with information that appears to be truthful but is actually false. For mitigations, see <a class="govuk-link" href="#scenario">developer uses LLM-generated code</a> without the need to review</p></li>
</ul>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="identityverificationusingimageandvideocapturetechnology">Identity verification using image and video capture technology</h4>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="scenario">Scenario</h4>
<p class="govuk-body">A government service requires users to prove their identity by capturing an image of an identity document containing their picture, such as a passport or driving licence. A CV AI system then compares this image to a live video clip of the person to verify that the person is who they claim to be. A malicious user uses AI deepfake technology to insert their own image over the genuine photo of another person on a stolen identity document. The CV system is tricked into wrongly verifying that the malicious user’s identity matches the credentials on the other user’s identity document.</p>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="impact">Impact</h4>
<ul class="govuk-list govuk-list--bullet govuk-list--spaced">
<li><p class="govuk-body">Misidentification of the true user, leading to identity fraud.</p></li>
<li><p class="govuk-body">Fraudulent access to a government service.</p></li>
<li><p class="govuk-body">Data loss of personal and confidential data about the genuine user.</p></li>
<li><p class="govuk-body">Serious security breach if the service provides access to sensitive government information.</p></li>
</ul>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="mitigation">Mitigation</h4>
<ul class="govuk-list govuk-list--bullet govuk-list--spaced">
<li><p class="govuk-body">Ensure the service only uses biometric identity documents. For example, biometric passports contain electronic passport photos which can be securely transferred to the service for verification.</p></li>
<li><p class="govuk-body">Use a trusted third-party service to look up and provide reference images for identity documents, rather than relying on images captured by users themselves. For example, the service could use licence images stored in the DVLA system and check this against the live video image.</p></li>
<li><p class="govuk-body">Use deepfake detection methods to scan input digital images and video clips which are received by the service.</p></li>
</ul>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="riskrating">Risk rating</h4>
<p class="govuk-body">Likelihood: MEDIUM</p>
<p class="govuk-body">Impact: HIGH</p>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="recommendation">Recommendation</h4>
<p class="govuk-body">In this specific example, use of a biometric passport can prevent the attack. However, if the deepfake were applied to the live video clip of the user instead of the image of the identity document, the system could still be fooled. Access to this type of deepfake technology is becoming increasingly available, meaning that when you build services to receive and process images or video, you must put in place mechanisms to detect if the content has been manipulated by AI.</p>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="mlopstoolsusedwithdefaultconfiguration">MLOps tools used with default configuration</h4>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="scenario-1">Scenario</h4>
<p class="govuk-body">A data scientist working in a government organisation wants to experiment with training their own ML model using organisational data. The experiment aims to test whether the model can be used to triage official correspondence, improving efficiency. The data scientist starts by using an open source MLOps tool to host and deploy their ML model in their local environment. The default configuration of the tool exposes a public endpoint with no authentication on the public internet. By default, the tool runs with administrator permissions on the host machine. A hacker discovers the exposed endpoint and sends commands to the MLOps tool, using it to gain a foothold in the organisation’s network.</p>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="impact-1">Impact</h4>
<ul class="govuk-list govuk-list--bullet govuk-list--spaced">
<li><p class="govuk-body">Serious security breach, which could lead to catastrophic damage to the organisation’s computer systems.</p></li>
<li><p class="govuk-body">Data loss, including the organisational data used to train the ML model, and other sensitive data that can be accessed through the tool’s elevated permissions.</p></li>
</ul>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="mitigation-1">Mitigation</h4>
<p class="govuk-body">Check the default configuration of all ML tools before deploying them and ensure basic security controls are in place. These include making sure that:</p>
<ul class="govuk-list govuk-list--bullet govuk-list--spaced">
<li><p class="govuk-body">authentication is enabled</p></li>
<li><p class="govuk-body">no public-facing endpoints are exposed unless explicitly required</p></li>
<li><p class="govuk-body">the principle of least privilege is applied so that tools only run with the minimum permissions they require</p></li>
</ul>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="riskrating-1">Risk rating</h4>
<p class="govuk-body">Likelihood: MEDIUM</p>
<p class="govuk-body">Impact: HIGH</p>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="recommendation-1">Recommendation</h4>
<p class="govuk-body">AI tools should be treated in the same way as all other third-party software. Even when they’re being used for experimentation, secure by design principles should always be applied.</p>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="llmchatbotonagovernmentwebsitefullchatinterface">LLM chatbot on a government website – full chat interface</h4>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="scenario-2">Scenario</h4>
<p class="govuk-body">A chatbot deployed to a government website to assist with queries relating to a particular public service. The chatbot uses a private instance of a publicly trained LLM. The user’s question is combined with system instructions that tell the LLM to only respond to questions relevant to the specific service. The system instructions are combined with the user’s original question and sent to the LLM. A malicious user could craft a specific prompt that circumvents the system instructions and makes the chatbot respond with irrelevant and potentially harmful information.</p>
<p class="govuk-body">This is an example of a direct prompt injection attack.</p>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="impact-2">Impact</h4>
<ul class="govuk-list govuk-list--bullet govuk-list--spaced">
<li><p class="govuk-body">Actual risk of user harm if a user is tricked into using an unsafe prompt that then results in harmful content being returned and acted on. For example, a user looking for information on how to pay a bill is directed to a fraudulent payment site.</p></li>
<li><p class="govuk-body">Reputational damage to the government if a user made public potentially harmful responses received from the chatbot. For example, when a user asks for generic information and receives an inflammatory response.</p></li>
</ul>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="mitigation-2">Mitigation</h4>
<ul class="govuk-list govuk-list--bullet govuk-list--spaced">
<li><p class="govuk-body">Use prompt engineering to attach a meta prompt to any user input to prevent the LLM from responding to malicious input.</p></li>
<li><p class="govuk-body">Apply content filters trained to detect likely prompt injections to all prompts sent to the LLM.</p></li>
<li><p class="govuk-body">Choose a more robust model as some models have been shown to be more resistant to this kind of attack than others.</p></li>
</ul>
<p class="govuk-body">None of these mitigations are sufficient to guarantee that a prompt injection attack would not succeed. Fundamentally, an LLM cannot distinguish between user input and system instructions. Both are processed by the LLM as natural language inputs so there is no way to prevent a user prompt affecting the behaviour of the LLM.</p>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="riskrating-2">Risk rating</h4>
<p class="govuk-body">Likelihood: HIGH</p>
<p class="govuk-body">Impact:</p>
<p class="govuk-body">LOW – response is returned to a single user with limited repercussions.</p>
<p class="govuk-body">HIGH – response causes actual harm to a user.</p>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="recommendation-2">Recommendation</h4>
<p class="govuk-body">Deploying an LLM chatbot to a public-facing government website comes with a significant risk of a direct prompt injection attack. You should consider the impact of an attack like this in the context of the specific use case. A chatbot deployed in a limited function or in controlled conditions, such as restricting the number of users, is far lower risk than one that is more widely available.</p>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="enterpriseaisearchtoolsummarisingemails">Enterprise AI search tool summarising emails</h4>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="scenario-3">Scenario</h4>
<p class="govuk-body">A hacker sends a malicious email attachment to a government recipient who is using an enterprise generative AI tool to assist them. The tool uses a RAG pattern, searching the private data the recipient can access and sending relevant data in-context to an LLM. The tool searches the recipient's inbox, including their unread emails and attachments, for relevant information. The tool passes the prompt injection contained in the attachment to the LLM along with other private data. The prompt injection causes the LLM to respond by summarising the private data in the form of an obfuscated link to a third-party website. When returned to the recipient, the link may, depending on the tools being used, automatically unfurl a preview and instantly exfiltrate the private data to the third-party website.</p>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="impact-3">Impact</h4>
<ul class="govuk-list govuk-list--bullet govuk-list--spaced">
<li><p class="govuk-body">Data loss where confidential information contained in the user’s emails is transferred to a third party.</p></li>
<li><p class="govuk-body">Reputational damage to the department due to loss of data.</p></li>
<li><p class="govuk-body">Regulatory breaches with financial consequences.</p></li>
</ul>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="mitigation-3">Mitigation</h4>
<ul class="govuk-list govuk-list--bullet govuk-list--spaced">
<li><p class="govuk-body">Configure the enterprise AI tool so that unread emails and attachments are not included in the initial search.</p></li>
<li><p class="govuk-body">Apply filters before the in-context data is added to the prompt to remove likely prompt injections.</p></li>
<li><p class="govuk-body">Apply filters to the response generated by the LLM to ensure any links contained in it are only to known resources.</p></li>
<li><p class="govuk-body">Ensure network controls are enforced that prevent applications making calls to dangerous URLs.</p></li>
</ul>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="riskrating-3">Risk rating</h4>
<p class="govuk-body">Likelihood: LOW</p>
<p class="govuk-body">Impact: HIGH</p>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="recommendation-3">Recommendation</h4>
<p class="govuk-body">In this scenario, indirect prompt injection in an email attachment can be used to perform data exfiltration without any action required by the user. Similar data exfiltration techniques have already been shown to work against commercial LLMs, such as this example of <a class="govuk-link" href="https://embracethered.com/blog/posts/2023/google-bard-data-exfiltration/">hacking Google Bard</a>. With the increased adoption by government departments of enterprise AI tools, we will likely see more of these novel generative AI-specific cybersecurity threats.</p>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="developerusesllmgeneratedcode">Developer uses LLM-generated code</h4>
<h5 class="govuk-body-s govuk-!-font-weight-bold" id="scenario-4">Scenario</h5>
<p class="govuk-body">A developer uses a public LLM to answer coding questions and receives advice to install a specific software package, ‘ArangoDB’, from the JavaScript package management system npm. When the LLM was trained, the package did not exist. A hacker has previously interrogated the LLM with common coding questions and identified this hallucination. They then created a malicious package with the fictitious name and registered it with the package management system. When the developer installs the package, they receive the malicious code.</p>
<h5 class="govuk-body-s govuk-!-font-weight-bold" id="impact-4">Impact</h5>
<ul class="govuk-list govuk-list--bullet govuk-list--spaced">
<li>Unauthorised code execution when the software containing the fake package is deployed and run. This could result in significant data loss and other serious consequences.</li>
</ul>
<h5 class="govuk-body-s govuk-!-font-weight-bold" id="mitigation-4">Mitigation</h5>
<ul class="govuk-list govuk-list--bullet govuk-list--spaced">
<li>Do not rely on the responses of the LLM. Double check all outputs before including them in your code. Check all package dependencies of your code before deployment. Use an automated tool such as a ‘dependabot’ or ‘snyk’ to scan for supply chain vulnerabilities.</li>
</ul>
<h5 class="govuk-body-s govuk-!-font-weight-bold" id="riskrating-4">Risk rating</h5>
<p class="govuk-body">Likelihood: LOW</p>
<p class="govuk-body">Impact: HIGH</p>
<h5 class="govuk-body-s govuk-!-font-weight-bold" id="recommendation-4">Recommendation</h5>
<p class="govuk-body">If developers follow secure coding best practices, the risk should never arise because all dependencies should be checked before deployment. Overreliance on LLM-generated code without sufficient human oversight is likely to become an increasing risk. Treat all LLM-generated code as inherently insecure and never use it directly in production code without first doing a code review.</p>
<h5 class="govuk-body-s govuk-!-font-weight-bold" id="references">References</h5>
<p class="govuk-body"><a class="govuk-link" href="https://vulcan.io/blog/ai-hallucinations-package-risk">Cybersecurity snapshot: new guide details how to use AI securely, as CERT honcho tells CISOs to sharpen AI security skills pronto</a></p>
<h3 class="govuk-heading-s" id="practicalrecommendations">Practical recommendations</h3>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="appliestoai">Applies to AI</h4>
<ul class="govuk-list govuk-list--bullet govuk-list--spaced">
<li><p class="govuk-body">Design risk-driven security, taking account of the failure modes for the type of AI you’re using. For example, the <a class="govuk-link" href="https://owasp.org/www-project-top-10-for-large-language-model-applications/">OWASP top 10 security risks for LLMs</a> or the <a class="govuk-link" href="https://atlas.mitre.org/matrices/ATLAS">ATLAS Matrix</a>.</p></li>
<li><p class="govuk-body">Use a consistent risk rating methodology to assess the impact and likelihood of each risk. For example, the <a class="govuk-link" href="https://owasp.org/www-community/OWASP_Risk_Rating_Methodology">OWASP risk rating methodology</a>.</p></li>
<li><p class="govuk-body">Minimise the attack surface by only using the AI capabilities you require. For example, by not sending user input directly to an LLM.</p></li>
<li><p class="govuk-body">Defend in depth by adding layers of security. For example, by using PET to prevent data leakage and adding content filters to sanitise output of an AI model.</p></li>
<li><p class="govuk-body">Never use private data that needs different levels of user access permissions to train or fine-tune an AI model.</p></li>
<li><p class="govuk-body">When building services that receive and process text, images or video, take steps to validate inputs to detect if the content has been generated by AI and could be unreliable, misleading or malicious.</p></li>
<li><p class="govuk-body">Review all enterprise access controls before deploying an AI tool to your environment to make sure users can only access the data they have permission to view.</p></li>
<li><p class="govuk-body">When experimenting with AI tools, pay attention to security and never assume default configurations are secure.</p></li>
</ul>
<h3 class="govuk-heading-s" id="practicalrecommendations-1">Practical recommendations</h3>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="appliestogenerativeai">Applies to generative AI</h4>
<ul class="govuk-list govuk-list--bullet govuk-list--spaced">
<li><p class="govuk-body">Avoid using generative AI where it’s not appropriate or required. Ask yourself if a non-AI solution or a traditional ML model trained for a specific purpose could work just as well.</p></li>
<li><p class="govuk-body">Prevent generative AI responses automatically leading to destructive or irreversible actions, such as sending emails or modifying records. In these situations a human must be present to review the action.</p></li>
<li><p class="govuk-body">Avoid using links to external resources in LLM responses that will be read by humans. If external links are provided, the response must be filtered to remove malicious URLs.</p></li>
<li><p class="govuk-body">Train your users not to trust the outputs of generative AI or rely exclusively on generated responses.</p></li>
<li><p class="govuk-body">Treat all LLM-generated code as inherently insecure and never use it directly in production without code review.</p></li>
<li><p class="govuk-body">Avoid putting LLM chatbots on public-facing government websites unless the risk of direct prompt injection is acceptable under the specific use case.</p></li>
<li><p class="govuk-body">When hosting virtual meetings, organisers should verify the identity of all attendees and state up front that the use of third-party AI meeting transcription tools is not allowed.</p></li>
</ul>
<h2 class="govuk-heading-m" id="usingaiwithgovernmentinformation">Using AI with government information</h2>
<p class="govuk-body">Many AI tools, especially generative AI tools, are available to support your team with time-consuming tasks such as working with big datasets, conducting research, drafting emails, and proof-reading.</p>
<p class="govuk-body">To mitigate the security risks identified later in this section, you must use these tools responsibly and in line with government security policies.</p>
<p class="govuk-body">All government information has a classification, even if not explicitly marked, and is covered by the <a class="govuk-link" href="https://www.gov.uk/government/publications/government-security-classifications">Government Security Classifications</a> policy (GSCP). This policy sets out how information should be protected, including the required security controls and expected behaviours.</p>
<p class="govuk-body">Both AI and generative AI tools that have been corporately assured by your organisation are appropriate to use when processing information in the OFFICIAL tier. This is because these tools will have been reviewed by cyber security, data protection, digital, and information management professionals, and have met government standards such as <a class="govuk-link" href="https://www.security.gov.uk/policy-and-guidance/secure-by-design/">Secure by Design</a> principles and the <a class="govuk-link" href="https://www.security.gov.uk/policy-and-guidance/the-cyber-security-standard/">Cyber Security Standard</a>.</p>
<p class="govuk-body">You must not use public or unassured generative AI tools to process OFFICIAL information that:</p>
<ul class="govuk-list govuk-list--bullet govuk-list--spaced">
<li><p class="govuk-body">could cause harm to the UK, individuals or our partners if compromised and has additional markings (for example information marked -SENSITIVE, -PERSONAL DATA, or -RECIPIENTS ONLY)</p></li>
<li><p class="govuk-body">contains personal data</p></li>
<li><p class="govuk-body">can affect the direction of a piece of work</p></li>
<li><p class="govuk-body">provides evidence of a material change in a situation</p></li>
</ul>
<p class="govuk-body">You must not process information held at SECRET and TOP SECRET tiers on official IT systems in any case.</p>
<p class="govuk-body">If you're unsure whether a tool is approved, or whether the information you're handling is appropriate for generative AI processing, speak to your organisation’s IT, data protection, or information governance team before proceeding.</p>
<h3 class="govuk-heading-s" id="practicalrecommendations-2">Practical recommendations</h3>
<ul class="govuk-list govuk-list--bullet govuk-list--spaced">
<li><p class="govuk-body">Ensure you use the right AI tool. AI tools (including generative AI) which have not undergone assurance processes must not be used to process any:</p></li>
<li><p class="govuk-body">OFFICIAL information with additional markings. This includes OFFICIAL information marked as -SENSITIVE, -PERSONAL DATA, or -RECIPIENTS ONLY.</p></li>
<li><p class="govuk-body">personal data</p></li>
<li><p class="govuk-body">information that can affect the direction of a piece of work</p></li>
<li><p class="govuk-body">information that provides evidence of a material change in a situation</p></li>
</ul></div> </div> </main> </div>  <footer data-astro-cid-sckkx6r4> <div class="govuk-width-container" data-astro-cid-sckkx6r4> <send-feedback data-astro-cid-sckkx6r4="true"></send-feedback> </div> <div class="govuk-footer" data-astro-cid-sckkx6r4> <div class="govuk-width-container" data-astro-cid-sckkx6r4> <div class="govuk-footer__meta" data-astro-cid-sckkx6r4> <div class="govuk-footer__meta-item govuk-footer__meta-item--grow" data-astro-cid-sckkx6r4> <h2 class="govuk-visually-hidden" data-astro-cid-sckkx6r4>Support links</h2> <ul class="govuk-footer__inline-list" data-astro-cid-sckkx6r4> <li class="govuk-footer__inline-list-item" data-astro-cid-sckkx6r4> <a class="govuk-footer__link" href="/site-map" data-astro-cid-sckkx6r4>Site map</a> </li> <li class="govuk-footer__inline-list-item" data-astro-cid-sckkx6r4> <a class="govuk-footer__link" href="/accessibility" data-astro-cid-sckkx6r4>Accessibility</a> </li> <li class="govuk-footer__inline-list-item" data-astro-cid-sckkx6r4> <a class="govuk-footer__link" href="mailto:ai-knowledge-hub@dsit.gov.uk" data-astro-cid-sckkx6r4>Contact us</a> </li> </ul> <svg aria-hidden="true" focusable="false" class="govuk-footer__licence-logo" viewBox="0 0 483.2 195.7" height="17" width="41" data-astro-cid-sckkx6r4> <path fill="currentColor" d="M421.5 142.8V.1l-50.7 32.3v161.1h112.4v-50.7zm-122.3-9.6A47.12 47.12 0 0 1 221 97.8c0-26 21.1-47.1 47.1-47.1 16.7 0 31.4 8.7 39.7 21.8l42.7-27.2A97.63 97.63 0 0 0 268.1 0c-36.5 0-68.3 20.1-85.1 49.7A98 98 0 0 0 97.8 0C43.9 0 0 43.9 0 97.8s43.9 97.8 97.8 97.8c36.5 0 68.3-20.1 85.1-49.7a97.76 97.76 0 0 0 149.6 25.4l19.4 22.2h3v-87.8h-80l24.3 27.5zM97.8 145c-26 0-47.1-21.1-47.1-47.1s21.1-47.1 47.1-47.1 47.2 21 47.2 47S123.8 145 97.8 145" data-astro-cid-sckkx6r4></path> </svg> <span class="govuk-footer__licence-description" data-astro-cid-sckkx6r4>
All content is available under the
<a class="govuk-footer__link" href="https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/" rel="license" data-astro-cid-sckkx6r4>Open Government Licence v3.0</a>, except where otherwise stated
</span> </div> <div class="govuk-footer__meta-item" data-astro-cid-sckkx6r4> <a class="govuk-footer__link govuk-footer__copyright-logo" href="https://www.nationalarchives.gov.uk/information-management/re-using-public-sector-information/uk-government-licensing-framework/crown-copyright/" data-astro-cid-sckkx6r4>© Crown copyright</a> </div> </div> </div> </div> </footer> <script type="module" src="/_astro/Layout.astro_astro_type_script_index_1_lang.DKaNH1GW.js"></script> </body></html> <script type="module">window.addEventListener("load",()=>{document.querySelectorAll(".analytics-on-this-page-link").forEach(e=>{e.addEventListener("click",n=>{posthog.capture("kh_contents_link_clicked",{guidance_page_name:n.currentTarget.textContent})})})});</script> <script type="module">const t=document.querySelector("on-this-page");window.addEventListener("message",function(e){typeof e.data?.content<"u"&&(document.querySelector("#content").innerHTML=e.data.content,document.querySelector("h1").textContent=e.data.title||"[Title goes here]",t.updateHeadings())});</script> <script type="module" src="/_astro/_...slug_.astro_astro_type_script_index_2_lang.Pa818wHY.js"></script>