<!DOCTYPE html><html lang="en" data-astro-cid-sckkx6r4> <head><meta charset="utf-8"><meta content="width=device-width, initial-scale=1.0" name="viewport"><link rel="icon" type="image/svg+xml" href="/favicon.svg"><link rel="preload" href="/fonts/static/Inter-Regular.ttf" as="font" type="font/ttf" crossorigin="anonymous"><link rel="preload" href="/fonts/static/Inter-Medium.ttf" as="font" type="font/ttf" crossorigin="anonymous"><title>
			Governance - 
			AI Knowledge Hub
		</title><meta property="og:title" content="Governance"><meta property="og:type" content="website"><meta property="og:url" content="https://ai.gov.uk/knowledge-hub/"><meta property="og:description" content="Find tools, prompts and how-to guides for teams adopting AI technologies in the UK government"><meta property="og:site_name" content="AI Knowledge Hub"><meta property="og:image" content="https://ai.gov.uk/img/knowledge-hub-banner.png"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:site" content="@i_dot_ai"><meta name="twitter:title" content="Governance"><meta name="twitter:description" content="Find tools, prompts and how-to guides for teams adopting AI technologies in the UK government"><meta name="twitter:image" content="https://ai.gov.uk/img/knowledge-hub-banner.png"><link href="/img/favicon.ico" rel="icon"><link href="/img/apple-touch-icon.png" rel="apple-touch-icon"><link rel="stylesheet" href="https://rsms.me/inter/inter.css"><script type="module" src="https://cdn.jsdelivr.net/gh/lit/dist@3/core/lit-core.min.js"></script><script type="module" src="/_astro/Layout.astro_astro_type_script_index_0_lang.uKs0OfS_.js"></script><script defer data-domain="ai.gov.uk" src="https://plausible.io/js/script.pageview-props.tagged-events.outbound-links.file-downloads.js"></script><script>
			!function(t,e){var o,n,p,r;e.__SV||(window.posthog=e,e._i=[],e.init=function(i,s,a){function g(t,e){var o=e.split(".");2==o.length&&(t=t[o[0]],e=o[1]),t[e]=function(){t.push([e].concat(Array.prototype.slice.call(arguments,0)))}}(p=t.createElement("script")).type="text/javascript",p.crossOrigin="anonymous",p.async=!0,p.src=s.api_host.replace(".i.posthog.com","-assets.i.posthog.com")+"/static/array.js",(r=t.getElementsByTagName("script")[0]).parentNode.insertBefore(p,r);var u=e;for(void 0!==a?u=e[a]=[]:a="posthog",u.people=u.people||[],u.toString=function(t){var e="posthog";return"posthog"!==a&&(e+="."+a),t||(e+=" (stub)"),e},u.people.toString=function(){return u.toString(1)+".people (stub)"},o="init Te Ds js Re Os As capture Ke calculateEventProperties zs register register_once register_for_session unregister unregister_for_session Hs getFeatureFlag getFeatureFlagPayload isFeatureEnabled reloadFeatureFlags updateEarlyAccessFeatureEnrollment getEarlyAccessFeatures on onFeatureFlags onSurveysLoaded onSessionId getSurveys getActiveMatchingSurveys renderSurvey displaySurvey canRenderSurvey canRenderSurveyAsync identify setPersonProperties group resetGroups setPersonPropertiesForFlags resetPersonPropertiesForFlags setGroupPropertiesForFlags resetGroupPropertiesForFlags reset get_distinct_id getGroups get_session_id get_session_replay_url alias set_config startSessionRecording stopSessionRecording sessionRecordingStarted captureException loadToolbar get_property getSessionProperty qs Ns createPersonProfile Bs Cs Ws opt_in_capturing opt_out_capturing has_opted_in_capturing has_opted_out_capturing get_explicit_consent_status is_capturing clear_opt_in_out_capturing Ls debug L Us getPageViewId captureTraceFeedback captureTraceMetric".split(" "),n=0;n<o.length;n++)g(u,o[n]);e._i.push([i,s,a])},e.__SV=1)}(document,window.posthog||[]);
			posthog.init('phc_6lt3zY0gpZ4Dldoae7ly31ufcffvjcrbR05YdLNyagL', { //pragma: allowlist secret
				cookieless_mode: 'always',	
				api_host: 'https://eu.i.posthog.com',
				defaults: '2025-05-24',
			})
		</script><link rel="stylesheet" href="/_astro/about.DUnlUy5V.css">
<style>.kh-back-to-top[data-astro-cid-qrl4wijt]{display:none}.kh-back-to-top__icon[data-astro-cid-qrl4wijt]{margin-right:10px}@media (min-width: 641px){.kh-back-to-top[data-astro-cid-qrl4wijt]{display:block;flex-shrink:0;padding-top:1.5rem;padding-bottom:.5rem}}
</style></head> <body class="govuk-template__body govuk-frontend-supported" data-astro-cid-sckkx6r4> <a class="govuk-skip-link" data-module="govuk-skip-link" href="#main" data-astro-cid-sckkx6r4>Skip to main content</a> <header> <div class="govuk-header govuk-header--full-width-border" data-module="govuk-header"> <div class="govuk-header__container govuk-width-container"> <div class="govuk-header__logo"> <a href="/" class="govuk-header__link govuk-header__link--homepage"> <span style="font-weight: bold;">AI.GOV.UK</span> <span class="govuk-header__product-name">Knowledge Hub</span> </a> </div> <div class="govuk-header__content"> <nav aria-label="Menu" class="govuk-header__navigation"> <button id="header-toggle-button" type="button" class="govuk-header__menu-button govuk-js-header-toggle" aria-controls="navigation" aria-expanded="false">Menu</button> <ul id="navigation" class="govuk-header__navigation-list"> <li class="govuk-header__navigation-item"> <a class="govuk-header__link" href="/about"> About </a> </li><li class="govuk-header__navigation-item"> <a class="govuk-header__link" href="/tools"> Tools </a> </li><li class="govuk-header__navigation-item"> <a class="govuk-header__link" href="/prompts"> Prompts </a> </li><li class="govuk-header__navigation-item"> <a class="govuk-header__link" href="/how-tos"> How-tos </a> </li> </ul> </nav> </div> </div> </div>  </header>  <div class="govuk-width-container"> <main id="main" class="govuk-main-wrapper govuk-!-padding-bottom-9 govuk-!-padding-top-7"> <nav class="govuk-breadcrumbs govuk-!-margin-top-0 govuk-!-margin-bottom-5" aria-label="Breadcrumb"> <ol class="govuk-breadcrumbs__list"> <li class="govuk-breadcrumbs__list-item"> <a class="govuk-breadcrumbs__link govuk-!-margin-0" href="/">Home</a> </li> <li class="govuk-breadcrumbs__list-item"> <a class="govuk-breadcrumbs__link govuk-!-margin-0" href="/how-tos">How-tos and resources</a> </li> </ol> </nav> <div class="govuk-grid-row"> <div class="govuk-grid-column-two-thirds govuk-!-margin-bottom-7 govuk-!-padding-bottom-5" data-pagefind-body> <h1 class="govuk-heading-xl">Governance (to be updated)</h1> <p class="govuk-body">
This information complements and updates the
<a href="https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government" class="govuk-link">
AI Playbook for the UK Government
</a>
. It reflects the latest guidance and best practice. This will be updated as needed.</p> </div> </div> <div class="govuk-grid-row"> <div class="govuk-grid-column-one-third kh-sticky-sidebar kh-sticky-sidebar--with-back-to-top"> <div class="kh-sticky-sidebar__nav"> <nav class="govuk-!-margin-bottom-6"> <h2 class="govuk-heading-s govuk-!-font-weight-regular kh-sticky-sidebar__nav-title">Contents</h2> <ul class="govuk-list govuk-list--spaced govuk-!-margin-bottom-0 kh-sticky-sidebar__nav-list"> <li class="kh-sticky-sidebar__list-item--dashed govuk-!-padding-right-5 govuk-!-padding-left-6"><a class="analytics-on-this-page-link" href="#aigovernanceboardorairepresentationonanexistingboard">AI governance board or AI representation on an existing board</a></li><li class="kh-sticky-sidebar__list-item--dashed govuk-!-padding-right-5 govuk-!-padding-left-6"><a class="analytics-on-this-page-link" href="#ethicscommittee">Ethics committee</a></li><li class="kh-sticky-sidebar__list-item--dashed govuk-!-padding-right-5 govuk-!-padding-left-6"><a class="analytics-on-this-page-link" href="#creatinganaisystemsinventory">Creating an AI systems inventory</a></li><li class="kh-sticky-sidebar__list-item--dashed govuk-!-padding-right-5 govuk-!-padding-left-6"><a class="analytics-on-this-page-link" href="#governancestructuresforteams">Governance structures for teams</a></li><li class="kh-sticky-sidebar__list-item--dashed govuk-!-padding-right-5 govuk-!-padding-left-6"><a class="analytics-on-this-page-link" href="#managingrisk">Managing risk</a></li><li class="kh-sticky-sidebar__list-item--dashed govuk-!-padding-right-5 govuk-!-padding-left-6"><a class="analytics-on-this-page-link" href="#aiqualityassurance">AI quality assurance</a></li> </ul> </nav> </div> <div class="kh-back-to-top" data-astro-cid-qrl4wijt> <a class="govuk-link govuk-link--no-visited-state" href="#top" data-astro-cid-qrl4wijt> <svg role="presentation" focusable="false" class="kh-back-to-top__icon" xmlns="http://www.w3.org/2000/svg" width="13" height="17" viewBox="0 0 13 17" data-astro-cid-qrl4wijt> <path fill="currentColor" d="M6.5 0L0 6.5 1.4 8l4-4v12.7h2V4l4.3 4L13 6.4z" data-astro-cid-qrl4wijt></path> </svg>Back to top
</a> </div>  </div> <div class="govuk-grid-column-two-thirds kh-guidance--draft" id="content"><p class="govuk-body">To successfully develop an AI programme, you’ll need strong governance processes because of the risks related to lawfulness, security, bias and data. Regardless of whether these processes are already built into your existing governance frameworks or implemented as a new governance framework, they should focus on:</p>
<ul class="govuk-list govuk-list--bullet govuk-list--spaced">
<li><p class="govuk-body">continuous improvement through the inclusion of new knowledge, methods and technologies</p></li>
<li><p class="govuk-body">identifying and working with important stakeholders representing different organisations and interests, including Civil Society Organisations (CSOs) and sector experts. This will help create a balanced view throughout the life cycle of any AI project or initiatives</p></li>
<li><p class="govuk-body">planning for the long-term sustainability of AI initiatives, considering scalability, long-term support, maintenance, ongoing stakeholder involvement and future developments</p></li>
</ul>
<p class="govuk-body">You should manage governance of AI through an AI governance board or AI expert representation on an existing governance board. You can include an ethics committee as part of your governance framework depending on your operating context. Each has different roles and responsibilities.</p>
<h3 class="govuk-heading-s" id="aigovernanceboardorairepresentationonanexistingboard">AI governance board or AI representation on an existing board</h3>
<p class="govuk-body">The role of an AI governance board or representation on a board is to provide oversight, accountability and strategic guidance to help the organisation or team make informed decisions about AI adoption and use. It covers aspects such as risk management, compliance, assurance, resource allocation, stakeholder engagement, and aligning with business objectives and ethical principles.</p>
<p class="govuk-body">An AI governance board helps you make sure your project is on track and that strategic, legal, ethical and operational risks are managed.</p>
<h3 class="govuk-heading-s" id="ethicscommittee">Ethics committee</h3>
<p class="govuk-body">The primary focus of an ethics committee is to assess the ethical implications of various actions, projects and decisions about AI within an organisation or programme. The committee will evaluate AI from an ethical standpoint, focusing on values such as fairness, transparency and privacy, and is more specialised than that of an AI governance board.</p>
<p class="govuk-body">An ethics committee usually includes legal experts, representatives from other relevant organisations related to the service you’re delivering and community members and stakeholders. All of these members can provide a specialised perspective on ethical matters such as health and security issues.</p>
<p class="govuk-body">Before creating an ethics committee, you should consider the ethical, strategic and operational context of your organisation or programme. For example, the department may be too small or the programme too low risk to have a committee like this. It might be sufficient to have an AI governance board or an AI expert representative on a programme board to help you manage ethical considerations. An AI governance board should be able to guide you on whether you need an ethics committee. Refer to the <a class="govuk-link" href="/how-to/ethics">ethics</a> section for more information.</p>
<h3 class="govuk-heading-s" id="creatinganaisystemsinventory">Creating an AI systems inventory</h3>
<p class="govuk-body">To provide a comprehensive view of all deployed AI systems within an organisation or programme, organisations should set up an AI and ML systems inventory. This is in addition to the <a class="govuk-link" href="https://www.gov.uk/government/collections/algorithmic-transparency-recording-standard-hub">Algorithmic Transparency Recording Standard Hub</a> that all government departments and certain arm’s length bodies must use to ensure public transparency around the algorithmic tools used in their decision-making processes.</p>
<p class="govuk-body">A live inventory will help your team, your organisation and your stakeholders understand the scope and scale of AI usage. It does this by providing better oversight and awareness of any potential risks such as data quality, model accuracy, bias, security vulnerabilities and regulatory compliance. An inventory will also be helpful for audit purposes.</p>
<p class="govuk-body">The inventory should be regularly kept up to date with:</p>
<ul class="govuk-list govuk-list--bullet govuk-list--spaced">
<li><p class="govuk-body">a description of each system’s purpose, usage and associated risks</p></li>
<li><p class="govuk-body">details like data elements, ownership, development and key dates</p></li>
<li><p class="govuk-body">use protocols, structures and tools for maintaining an accurate, detailed inventory</p></li>
</ul>
<p class="govuk-body">You should consider sharing your inventory with the <a class="govuk-link" href="https://www.gov.uk/service-manual/communities/artificial-intelligence-community">AI community of practice</a>. This will enable the community to support your work and connect you with the teams that have developed similar projects across government so that you can share expertise and best practices and possibly reuse existing solutions.</p>
<h3 class="govuk-heading-s" id="governancestructuresforteams">Governance structures for teams</h3>
<p class="govuk-body">For all programmes or services that use AI systems, teams should:</p>
<ul class="govuk-list govuk-list--bullet govuk-list--spaced">
<li><p class="govuk-body">set out how the AI model will be maintained and managed over time</p></li>
<li><p class="govuk-body">develop a comprehensive plan for knowledge transfer, and for training new and existing staff to ensure the model’s sustainable management</p></li>
<li><p class="govuk-body">establish clear roles and responsibilities to ensure accountability within teams, including who has the authority to change and modify the code of the AI model</p></li>
<li><p class="govuk-body">ensure diversity within the project team by incorporating a range of subject matter expertise, skills and lived experiences</p></li>
<li><p class="govuk-body">establish pathways for escalation and identify key points of contact for specific AI-related issues</p></li>
<li><p class="govuk-body">adopt a risk prioritisation plan with specific project controls throughout the delivery and post-delivery cycle, such as how you will evaluate data sets for bias</p></li>
<li><p class="govuk-body">establish a data reporting mechanism that captures how data flows are managed and maintained throughout the delivery and post-delivery cycle</p></li>
<li><p class="govuk-body">set out how the programme or project team will work with and report to their programme boards and the ethics committee, if one has been set up</p></li>
</ul>
<h3 class="govuk-heading-s" id="managingrisk">Managing risk</h3>
<p class="govuk-body">Risk management is part of governance. It helps you to strategically plan and manage your AI project to achieve objectives and respond to challenges in an agile way.</p>
<p class="govuk-body">A risk assessment is critical in ensuring that AI projects are only undertaken if the potential benefits outweigh the risks. You should base this threshold on an objective assessment of the project's potential risks and benefits, and by defining acceptable levels of risk and ensuring that any possible risks are identified and addressed early in the project life cycle. Relevant laws, regulations and ethical considerations should inform the assessment.</p>
<p class="govuk-body">If you’re managing AI programmes as part of a portfolio of work, the <a class="govuk-link" href="https://www.gov.uk/government/publications/orange-book">Orange Book</a> and its <a class="govuk-link" href="https://assets.publishing.service.gov.uk/media/6453b363c33b460012f5e6bf/Portfolio_Risk_Management_Guidance_Orange_Book_Annex.pdf">Portfolio Risk Management Guidance</a> provide a complete overview of risks. You can use the <a class="govuk-link" href="https://www.gov.uk/government/publications/orange-book/quick-guide-central-governments-assurance-directory">Central Government’s Assurance Directory (CGAD)</a> guide to navigate the numerous standards, codes and guidance documents currently issued by the centre of government which complement the Orange book. You should also refer to the Cabinet Office guidance on <a class="govuk-link" href="https://www.gov.uk/government/publications/a-human-centred-approach-to-scaling-and-de-risking-ai-tools">a human-centred approach to scaling and de-risking AI tools</a> to consider the hidden risk of AI in light of the cultural, organisational, and human factors that can influence AI adoption and design.</p>
<p class="govuk-body">As part of the design of your project, you should conduct a risk assessment to understand the risks and their potential to cause harm to individuals or groups, as well as the likelihood of the AI service being misused or exploited. The impact should be calculated based on the complexity of the AI system, the quality of the data used to train the system, and the potential for human error or malicious intent.</p>
<p class="govuk-body">When conducting your assessment, you should consider a number of risks around AI including security, managing bias, legal and operational risks. Consider also that the scale of autonomy of an AI service can increase operational risks. For example, in the case of autonomous vehicles, the Society of Automotive Engineers’ <a class="govuk-link" href="https://www.sae.org/blog/sae-j3016-update">Levels of Driving Automation</a> ranks autonomy on a scale from 0 (no autonomy) to 5 (full automation for all features under all conditions). This scale correlates to the level of risk.</p>
<p class="govuk-body">Alongside the risk assessment, you need to create a robust risk management framework that sets out defined roles and responsibilities and includes clear escalation routes to help mitigate risks.</p>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="mitigatingrisks">Mitigating risks</h4>
<p class="govuk-body">You can mitigate some risks related to how the AI service performs by building or establishing programme and technical guardrails, or best practices. These will guide the design, implementation and operation of an AI service or application, and are an essential element of delivering great services.</p>
<p class="govuk-body">In the case of autonomous AI services that make decisions in areas such as social care or healthcare, the impact of autonomy of an AI service can be mitigated by including human intervention. These decisions need to be made in a controlled environment so as to not reintroduce bias into the AI service.</p>
<p class="govuk-body">Whether your AI service is autonomous or includes elements of human intervention, it should be evaluated throughout the all stages of the project life cycle, including design, development and operation. Your risks and mitigation strategy should also cover how your team will manage continuous performance monitoring to prevent biased or inaccurate outputs.</p>
<p class="govuk-body">There are also security and data protection risks which are covered in detail in the AI security risks data protection and privacy sections.</p>
<h3 class="govuk-heading-s" id="aiqualityassurance">AI quality assurance</h3>
<p class="govuk-body">AI quality assurance ensures that the AI service meets the service level requirements and provides evidence that the service is fit for purpose. It helps you to check that robust techniques have been used to build, test, measure and evaluate AI systems. It also helps organisations communicate that their systems are trustworthy and aligned with relevant regulatory principles. It should be used throughout the AI service life cycle, including during testing and validation in the development phase and monitoring once the AI service is being used.</p>
<p class="govuk-body">To meet quality assurance requirements, AI systems must be trustworthy, accountable, transparent and robust. They must ensure safety, respect privacy, mitigate bias, ensure fairness, and be secure and resilient. Given the complexity of AI systems, you may require a toolbox of different products, services and standards to ensure their effectiveness. For example, DSIT’s <a class="govuk-link" href="https://assets.publishing.service.gov.uk/media/65ccf508c96cf3000c6a37a1/Introduction_to_AI_Assurance.pdf">introduction to AI assurance</a> identifies the key elements of an assurance process which includes risk assessment, impact assessment, bias audit, compliance audit, conformity assessment and formal verification. To ensure your AI tools are properly tested and assured, you can also use the <a class="govuk-link" href="https://eur02.safelinks.protection.outlook.com/?url=https%3A%2F%2Ftesting-ai-standards.github.io%2Fcross-gov-ai-testing-framework%2F&data=05%7C02%7CTommaso.Spinelli%40dsit.gov.uk%7Cd6e04c5b3ccb4011434808ddbff50fd6%7Ccbac700502c143ebb497e6492d1b2dd8%7C0%7C0%7C638877781455157919%7CUnknown%7CTWFpbGZsb3d8eyJFbXB0eU1hcGkiOnRydWUsIlYiOiIwLjAuMDAwMCIsIlAiOiJXaW4zMiIsIkFOIjoiTWFpbCIsIldUIjoyfQ%3D%3D%7C0%7C%7C%7C&sdata=Szg1P4UiaHXV1orQzi%2BdFFSjFvR3xSnC%2FAIVLx3Vxcs%3D&reserved=0">AI Testing and Assurance Framework for Public Sector</a> designed by the Cross-Government Testing Community.</p>
<p class="govuk-body">The <a class="govuk-link" href="https://assets.publishing.service.gov.uk/media/5e96cab9d3bf7f412b2264b1/HMT_Magenta_Book.pdf">Magenta Book</a> and associated <a class="govuk-link" href="https://assets.publishing.service.gov.uk/media/672c84ebbd79990dfa67cab4/2024-11-05_Guidance_on_the_impact_evaluation_of_AI_interventions_FINAL_PDF_WITH_ACCESSIBILITY_CHANGES.pdf">guidance on the impact evaluation of AI interventions</a> offer a structured, evidence-based approach to assessing the effectiveness and value of AI systems. Integrating evaluation activities throughout the AI lifecycle supports the development of trustworthy systems by ensuring that your services are built using robust methods and are continuously assessed for performance, unintended consequences and real-world impact.</p>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="validationofai">Validation of AI</h4>
<p class="govuk-body">Being able to assert the quality of an AI service is critical in ensuring both the safety of the system and the reliable accuracy of the service.</p>
<p class="govuk-body">You must be sure that any updates to the AI system have a quantitative testing and validation process as part of the change control process. Validation is part of the testing of an AI system. <a class="govuk-link" href="https://www.iso.org/standard/45481.html">ISO9000:2015 - Quality management systems</a> specifies that it is ‘confirmation, through the provision of objective evidence, that the requirements for a specific intended use or application have been fulfilled’.</p>
<p class="govuk-body">The deployment of AI systems that are inaccurate, unreliable, or poorly generalised to data beyond their training creates and increases negative AI risks and reduces trustworthiness. You should consider the complexity of your AI systems and identify the different products, services and standards necessary to ensure their effectiveness. To do so, you must also make sure that these products and services comply with the required standards as defined by standards development organisations (SDOs), such as the <a class="govuk-link" href="https://www.iso.org/home.html">International Standards Organisation (ISO)</a>.</p>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="operationalmonitoring">Operational monitoring</h4>
<p class="govuk-body">Once you’ve released your AI system for use and it’s operational, you should have ongoing performance monitoring in place. This will ensure your system is operating as expected, and you should be able to provide evidence of this. It will also help you to identify and manage any changes to the model.</p>
<p class="govuk-body">Any updates to the model need to go through a managed release process. This will help you to mitigate the impact of any process change and to clearly document changes made for future reference. You should ensure that the release can be withdrawn and the system reverted to an earlier version if required.</p>
<p class="govuk-body">As systems and environments evolve, the current process may diverge sufficiently from the training period of the AI system. This is known as model drift and may require retraining or implementation of a new model within the AI system. Close monitoring is essential so that you can catch this as early as possible and reduce possible disruption to the AI system.</p>
<h4 class="govuk-body-s govuk-!-font-weight-bold" id="practicalrecommendations">Practical recommendations</h4>
<ul class="govuk-list govuk-list--bullet govuk-list--spaced">
<li><p class="govuk-body">Connect with your organisation’s assurance team and review the <a class="govuk-link" href="https://www.gov.uk/guidance/cdei-portfolio-of-ai-assurance-techniques">portfolio of AI assurance techniques</a>.</p></li>
<li><p class="govuk-body">Set up an AI governance board or include AI experts on existing governance boards.</p></li>
<li><p class="govuk-body">Consider setting up an ethics committee made up of internal stakeholders, cross-government stakeholders, sector experts and external stakeholders like Civil Society Organisations.</p></li>
<li><p class="govuk-body">Set up an AI/ML systems inventory to provide a comprehensive view of all deployed AI systems within your department.</p></li>
<li><p class="govuk-body">Make sure your programme or project teams have clear governance structures in place.</p></li>
<li><p class="govuk-body">Conduct an assessment of your AI product to identify potential risks and implement a robust mitigation strategy.</p></li>
<li><p class="govuk-body">Use quality assurance techniques to make sure your AI product is trustworthy, accountable, transparent, robust, secure and resilient, and that it respects privacy, mitigates bias and ensures fairness.</p></li>
<li><p class="govuk-body">Conduct evaluation activities in accordance with the <a class="govuk-link" href="https://assets.publishing.service.gov.uk/media/5e96cab9d3bf7f412b2264b1/HMT_Magenta_Book.pdf">Magenta Book</a>.</p></li>
<li><p class="govuk-body">Make full use of the training resources available, including the courses on <a class="govuk-link" href="https://learn.civilservice.gov.uk/courses/oPEgywmEQumlVAmXlgnRqw">the business value of AI</a> and <a class="govuk-link" href="https://learn.civilservice.gov.uk/courses/f7Sf3JPkTQiwYgr2qatDEw">understanding AI ethics</a> on Civil Service Learning.</p></li>
</ul></div> </div> </main> </div>  <footer data-astro-cid-sckkx6r4> <div class="govuk-width-container" data-astro-cid-sckkx6r4> <send-feedback data-astro-cid-sckkx6r4="true"></send-feedback> </div> <div class="govuk-footer" data-astro-cid-sckkx6r4> <div class="govuk-width-container" data-astro-cid-sckkx6r4> <div class="govuk-footer__meta" data-astro-cid-sckkx6r4> <div class="govuk-footer__meta-item govuk-footer__meta-item--grow" data-astro-cid-sckkx6r4> <h2 class="govuk-visually-hidden" data-astro-cid-sckkx6r4>Support links</h2> <ul class="govuk-footer__inline-list" data-astro-cid-sckkx6r4> <li class="govuk-footer__inline-list-item" data-astro-cid-sckkx6r4> <a class="govuk-footer__link" href="/site-map" data-astro-cid-sckkx6r4>Site map</a> </li> <li class="govuk-footer__inline-list-item" data-astro-cid-sckkx6r4> <a class="govuk-footer__link" href="/accessibility" data-astro-cid-sckkx6r4>Accessibility</a> </li> <li class="govuk-footer__inline-list-item" data-astro-cid-sckkx6r4> <a class="govuk-footer__link" href="mailto:ai-knowledge-hub@dsit.gov.uk" data-astro-cid-sckkx6r4>Contact us</a> </li> </ul> <svg aria-hidden="true" focusable="false" class="govuk-footer__licence-logo" viewBox="0 0 483.2 195.7" height="17" width="41" data-astro-cid-sckkx6r4> <path fill="currentColor" d="M421.5 142.8V.1l-50.7 32.3v161.1h112.4v-50.7zm-122.3-9.6A47.12 47.12 0 0 1 221 97.8c0-26 21.1-47.1 47.1-47.1 16.7 0 31.4 8.7 39.7 21.8l42.7-27.2A97.63 97.63 0 0 0 268.1 0c-36.5 0-68.3 20.1-85.1 49.7A98 98 0 0 0 97.8 0C43.9 0 0 43.9 0 97.8s43.9 97.8 97.8 97.8c36.5 0 68.3-20.1 85.1-49.7a97.76 97.76 0 0 0 149.6 25.4l19.4 22.2h3v-87.8h-80l24.3 27.5zM97.8 145c-26 0-47.1-21.1-47.1-47.1s21.1-47.1 47.1-47.1 47.2 21 47.2 47S123.8 145 97.8 145" data-astro-cid-sckkx6r4></path> </svg> <span class="govuk-footer__licence-description" data-astro-cid-sckkx6r4>
All content is available under the
<a class="govuk-footer__link" href="https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/" rel="license" data-astro-cid-sckkx6r4>Open Government Licence v3.0</a>, except where otherwise stated
</span> </div> <div class="govuk-footer__meta-item" data-astro-cid-sckkx6r4> <a class="govuk-footer__link govuk-footer__copyright-logo" href="https://www.nationalarchives.gov.uk/information-management/re-using-public-sector-information/uk-government-licensing-framework/crown-copyright/" data-astro-cid-sckkx6r4>© Crown copyright</a> </div> </div> </div> </div> </footer> <script type="module" src="/_astro/Layout.astro_astro_type_script_index_1_lang.DKaNH1GW.js"></script> </body></html> <script type="module">window.addEventListener("load",()=>{document.querySelectorAll(".analytics-on-this-page-link").forEach(e=>{e.addEventListener("click",n=>{posthog.capture("kh_contents_link_clicked",{guidance_page_name:n.currentTarget.textContent})})})});</script>